# CAFA-6 Protein Function Prediction (AI-driven Development)

This repository is designed for **AI-driven development** of a protein function
prediction system for the Kaggle CAFA-6 competition.

**Competition Link**: https://www.kaggle.com/competitions/cafa-6-protein-function-prediction/overview

---

## ğŸ¯ Project Goal

Predict Gene Ontology (GO) terms for proteins using:

- Amino acid sequences
- (Optional) taxonomy / species information


---

## âŒ Non-Goals (Important)

This repository is **NOT** intended to:

- Train very large end-to-end models from scratch
- Perform heavy hyperparameter searches inside Kaggle
- Mix experimental notebook code with production code

---

## ğŸ§  AI Development Policy

When modifying or extending this repository, **AI agents must follow these rules**:

1. All reusable logic must be implemented under `src/`
2. Notebooks are for exploration only (no core logic)
3. File paths and parameters must be configurable via `configs/`
4. Cached artifacts (embeddings, models) must be saved explicitly
5. Code should favor clarity and debuggability over extreme optimization

---

## ğŸ“¦ Dataset Overview

The dataset is designed for **multi-label protein function prediction** using Gene Ontology (GO) terms.

### Key Files

| File | Description |
|------|-------------|
| `train_sequences.fasta` | Protein amino acid sequences (training) |
| `train_terms.tsv` | Ground truth GO term annotations |
| `train_taxonomy.tsv` | Taxonomy (species) information for proteins |
| `go-basic.obo` | Gene Ontology hierarchy structure |
| `testsuperset.fasta` | Test protein sequences for prediction |
| `testsuperset-taxon-list.tsv` | Taxonomy IDs for test proteins |
| `IA.tsv` | Information Accretion weights for GO terms |
| `sample_submission.tsv` | Submission format template |

### Important Notes

- **Multi-label problem**: Each protein can have multiple GO terms
- **Hierarchical structure**: GO terms are organized in an ontology
- **Taxonomy is optional**: Species information may improve predictions

### Data Statistics (from `src/main.ipynb` analysis)

**Training Sequences:**
- Number of sequences: 82,403
- Sequence length distribution:
  - Min: 16 amino acids
  - Max: 10,000+ amino acids
  - Mean: ~400-500 amino acids
  - 25th percentile: ~200 aa
  - 50th percentile (median): ~350 aa
  - 75th percentile: ~550 aa
  - 90th percentile: ~800 aa
  - 95th percentile: ~1,100 aa
  - 99th percentile: ~2,000 aa
- Long sequences (>10,000 aa): A small number exist

**Test Data:**
- Species distribution (from `testsuperset-taxon-list.tsv`):
  - Human (Homo sapiens, 9606)
  - Rat (Rattus norvegicus, 10116)
  - Rice (Oryza sativa subsp. japonica, 39947)
  - Zebrafish (Danio rerio, 7955)
  - Fruit fly (Drosophila melanogaster, 7227)

**GO Terms:**
- GO ontology structure includes:
  - `id`: GO term identifier (e.g., GO:0000001)
  - `name`: Term name (e.g., "mitochondrion inheritance")
  - `namespace`: biological_process, molecular_function, or cellular_component
  - `is_a`: Parent-child relationships in the ontology
  - `def`: Definition with references

**Submission Format:**
- The submission file contains both GO term predictions and free-text descriptions
- Two types of rows per protein:
  - GO term rows: `protein_id`, `GO:XXXXXXX`, `score` (0-1)
  - Text description rows: `protein_id`, `Text`, `score`, `description`


## ğŸ“ Project Structure

```
Kaggle_CAFA6/
â”œâ”€â”€ input/                          # Competition data files
â”‚   â””â”€â”€ cafa-6-protein-function-prediction/
â”‚       â”œâ”€â”€ Train/                  # Training data
â”‚       â”‚   â”œâ”€â”€ train_sequences.fasta
â”‚       â”‚   â”œâ”€â”€ train_terms.tsv
â”‚       â”‚   â”œâ”€â”€ train_taxonomy.tsv
â”‚       â”‚   â””â”€â”€ go-basic.obo
â”‚       â”œâ”€â”€ Test (Targets)/         # Test data
â”‚       â”‚   â”œâ”€â”€ testsuperset.fasta
â”‚       â”‚   â””â”€â”€ testsuperset-taxon-list.tsv
â”‚       â”œâ”€â”€ IA.tsv                  # GO term Information Accretion weights
â”‚       â””â”€â”€ sample_submission.tsv
â”œâ”€â”€ src/                            # Source code (modularized)
â”‚   â”œâ”€â”€ main.py                     # Main pipeline script
â”‚   â”œâ”€â”€ config.py                   # Configuration and paths
â”‚   â”œâ”€â”€ data_loader.py              # Data loading utilities
â”‚   â”œâ”€â”€ protein_embedding.py        # ESM-2 protein embedding
â”‚   â”œâ”€â”€ go_embedding.py             # BiomedBERT GO term embedding
â”‚   â”œâ”€â”€ model.py                    # JointModel architecture
â”‚   â”œâ”€â”€ training.py                 # Training loop and checkpointing
â”‚   â”œâ”€â”€ prediction.py               # Inference and submission
â”‚   â”œâ”€â”€ hierarchical_postprocess.py # Hierarchical GO postprocessing
â”‚   â”œâ”€â”€ evaluation.py               # Evaluation metrics (IA-weighted Fmax)
â”‚   â””â”€â”€ main.ipynb                  # Legacy notebook (archived)
â”œâ”€â”€ docs/                           # Documentation
â”‚   â”œâ”€â”€ data_loading.md             # Data loading details
â”‚   â”œâ”€â”€ embeddings.md               # Embedding generation
â”‚   â”œâ”€â”€ model_architecture.md       # JointModel structure
â”‚   â”œâ”€â”€ hierarchical_postprocessing.md  # Postprocessing details
â”‚   â””â”€â”€ evaluation.md               # Evaluation metrics (CAFA-6)
â”œâ”€â”€ model/                          # Pre-trained model weights (gitignored)
â”‚   â”œâ”€â”€ esm2_t12_35M_UR50D/        # ESM-2 protein embedding model
â”‚   â””â”€â”€ BiomedNLP-BiomedBERT-base-uncased-abstract-fulltext/
â”œâ”€â”€ output/                         # Generated files
â”‚   â”œâ”€â”€ embeddings/                 # Cached embeddings
â”‚   â”‚   â”œâ”€â”€ train_protein_embeddings.pt
â”‚   â”‚   â”œâ”€â”€ test_protein_embeddings.pt
â”‚   â”‚   â””â”€â”€ go_embeddings.pt
â”‚   â”œâ”€â”€ models/                     # Trained models
â”‚   â”‚   â””â”€â”€ joint_model.pt
â”‚   â””â”€â”€ submission.tsv              # Final submission file
â”œâ”€â”€ pyproject.toml                  # Project dependencies (uv)
â””â”€â”€ README.md
```

---

## ğŸ§  Modeling Approach

This repository adopts a **dual-embedding (dual-encoder) approach** for protein
function prediction.

Instead of directly classifying proteins into a fixed set of GO labels,
we embed **proteins** and **GO terms** into a shared latent space and train a model
to align them.

---

### 1. Protein Embedding

- Each protein is represented by its amino acid sequence.
- Sequences are encoded using a pretrained protein language model
  (e.g. ESM-2).
- The output is a fixed-dimensional vector representing the protein.

This embedding captures:
- sequence patterns
- evolutionary and structural signals
- biochemical properties learned from large protein databases

---

### 2. GO Term Embedding

- Each Gene Ontology (GO) term is also represented as a vector.
- GO embeddings are obtained separately (e.g. from text descriptions,
  ontology structure, or a pretrained language model).
- All GO terms are embedded into the same dimensional space.

This allows the model to:
- reason about similarities between GO terms
- generalize across related biological functions

---

### 3. Shared Latent Space

Both protein embeddings and GO embeddings are projected into a **shared latent space**
using lightweight neural networks (typically linear layers).

Let:
- `h_p` be the protein embedding
- `h_go` be the GO term embedding

Then:
- `z_p = f_p(h_p)`  
- `z_go = f_go(h_go)`

where `f_p` and `f_go` are learnable projection functions.

---

### 4. Training Objective

The model is trained to **bring related proteinâ€“GO pairs closer together**
in the latent space, while pushing unrelated pairs further apart.

Intuitively:
- If a protein is annotated with a GO term, their embeddings should be similar.
- If not, their embeddings should be dissimilar.

This alignment objective can be implemented using:
- similarity scores (e.g. dot product or cosine similarity)
- multi-label losses (e.g. BCE over proteinâ€“GO pairs)

---

### 5. Inference

At inference time:
1. Embed the protein sequence.
2. Compare it against all GO term embeddings.
3. Rank GO terms by similarity score.
4. Output GO terms and confidence scores in Kaggle submission format.

---

### Design Rationale

This embedding-based formulation offers several advantages:

- Scalability to large GO vocabularies
- Flexibility to incorporate GO semantics
- Efficient reuse of cached embeddings
- Clear separation between representation learning and prediction logic

This design is particularly suitable for:
- AI-assisted iterative development
- constrained execution environments (e.g. Kaggle notebooks)

---

## ğŸš€ Implementation Status & Pipeline

### Current Pipeline (8 Steps)

The implementation is fully modularized into `src/*.py` files. The main pipeline in [src/main.py](src/main.py) executes the following steps:

**Step 1: Data Loading**
- Load FASTA sequences (train/test) using [data_loader.py](src/data_loader.py)
- Parse GO ontology from `go-basic.obo` to extract hierarchy (`child_to_parents`, `go_terms`)
- Load training labels from `train_terms.tsv`

**Step 2: Protein Embeddings (ESM-2)**
- Encode protein sequences using ESM-2 (`facebook/esm2_t12_35M_UR50D`)
- Implementation: [protein_embedding.py](src/protein_embedding.py)
- Output: 480-dimensional vectors
- Cached to disk for reuse (`.pt` files in `output/embeddings/`)

**Step 3: GO Term Embeddings (BiomedBERT)**
- Encode GO term names and definitions using BiomedBERT
- Model: `microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract-fulltext`
- Implementation: [go_embedding.py](src/go_embedding.py)
- Output: 768-dimensional vectors

**Step 4: Train/Validation Split**
- Stratified split by label count (default: 80/20)
- Implementation: [evaluation.py](src/evaluation.py) â†’ `split_train_validation()`
- Ensures balanced distribution of protein annotation complexity

**Step 5: Model Training**
- **Architecture**: JointModel (Dual-Encoder)
  - Protein encoder: Linear(480 â†’ 256)
  - GO encoder: Linear(768 â†’ 256)
  - Scoring: Dot product in joint space
- **Loss**: BCEWithLogitsLoss (multi-label classification)
- **Optimizer**: Adam (lr=1e-3)
- Implementation: [model.py](src/model.py), [training.py](src/training.py)

**Step 6: Validation Evaluation**
- Metrics computed on validation set:
  - Basic: Precision, Recall, F1, Average Precision
  - **IA-weighted Fmax**: Official CAFA-6 metric
- Implementation: [evaluation.py](src/evaluation.py)

**Step 7: Prediction on Test Set**
- Encode test proteins â†’ compute scores for all GO terms
- Select top-K predictions per protein (default: K=100)
- Implementation: [prediction.py](src/prediction.py)

**Step 7.5: Hierarchical Postprocessing (Optional)**
- Enforces GO hierarchy constraints using hybrid approach:
  1. **Bottom-up propagation**: If child has high score, increase parent score (Î±=0.3)
  2. **Top-down suppression**: If parent has low score, decrease child score (threshold=0.3, Î²=0.5)
- Implementation: [hierarchical_postprocess.py](src/hierarchical_postprocess.py)
- Toggle: `ENABLE_HIERARCHICAL_POSTPROCESS` in [config.py](src/config.py)

**Step 8: Submission File Creation**
- Generate `submission.tsv` in Kaggle submission format
- Format: `protein_id\tGO:term\tscore` (one per line)

### Development Mode (DEV_TEST)

For faster iteration during development, set `DEV_TEST = True` in [config.py](src/config.py):

```python
# config.py
DEV_TEST = True  # Enables development mode
DEV_TEST_MAX_BATCHES = 100  # Process only first 100 batches
```

**Effects:**
- Limits ESM-2 encoding to first 100 batches (faster testing)
- Uses separate output files with `_dev` suffix to avoid overwriting production outputs
- Ideal for rapid prototyping and debugging

**Usage:**
```bash
# Development mode - quick testing
python src/main.py  # with DEV_TEST=True in config.py

# Production mode - full dataset
python src/main.py  # with DEV_TEST=False in config.py
```

### Configuration

All parameters are centralized in [src/config.py](src/config.py):

```python
# Model parameters
JOINT_DIM = 256
TRAIN_BATCH_SIZE = 16
NUM_EPOCHS = 10
LEARNING_RATE = 1e-3

# Hierarchical postprocessing
ENABLE_HIERARCHICAL_POSTPROCESS = True
HIERARCHICAL_ALPHA = 0.3
HIERARCHICAL_THRESHOLD = 0.3
HIERARCHICAL_BETA = 0.5

# Evaluation
ENABLE_VALIDATION = True
VAL_SPLIT_RATIO = 0.2
VAL_STRATIFY_BY_LABEL_COUNT = True
```

### Documentation

Detailed documentation for each component is available in the `docs/` directory:

- [Data Loading](docs/data_loading.md): FASTA parsing, GO OBO parsing, label loading
- [Embeddings](docs/embeddings.md): ESM-2 protein encoding, BiomedBERT GO encoding
- [Model Architecture](docs/model_architecture.md): JointModel structure, training, inference
- [Hierarchical Postprocessing](docs/hierarchical_postprocessing.md): Bottom-up/Top-down algorithms, parameters
- [Evaluation](docs/evaluation.md): IA-weighted Fmax, validation metrics (CAFA-6 official)

---

## ğŸ› ï¸ ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—æ‰‹é †

### 1. ç’°å¢ƒæ§‹ç¯‰

1. **uvã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«**
   ```bash
   curl -LsSf https://astral.sh/uv/install.sh | sh
   uv --version
   ```

2. **ä»®æƒ³ç’°å¢ƒä½œæˆ**
   ```bash
   uv venv
   ```

3. **ä»®æƒ³ç’°å¢ƒæœ‰åŠ¹åŒ–**
   ```bash
   source .venv/bin/activate
   ```

4. **ä¾å­˜é–¢ä¿‚ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«**
   ```bash
   uv pip install -e .
   ```

### 2. ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰

Kaggleã‹ã‚‰ã‚³ãƒ³ãƒšãƒ†ã‚£ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã€`input/cafa-6-protein-function-prediction/`ã«é…ç½®ã—ã¦ãã ã•ã„ã€‚

```bash
# Kaggle CLIã‚’ä½¿ç”¨ã™ã‚‹å ´åˆ
kaggle competitions download -c cafa-6-protein-function-prediction
unzip cafa-6-protein-function-prediction.zip -d input/cafa-6-protein-function-prediction/
```

### 3. ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

ã“ã®ãƒ¬ãƒã‚¸ãƒˆãƒªã§ã¯ã€äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã¯**Gitã§ç®¡ç†ã—ã¦ã„ã¾ã›ã‚“**ï¼ˆã‚µã‚¤ã‚ºãŒå¤§ãã„ãŸã‚ï¼‰ã€‚
ä»¥ä¸‹ã®æ‰‹é †ã§ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æº–å‚™ã—ã¦ãã ã•ã„ã€‚

#### ã‚ªãƒ—ã‚·ãƒ§ãƒ³A: Hugging Face Hubã‹ã‚‰ç›´æ¥ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆæ¨å¥¨ï¼‰

```python
from transformers import AutoModel, AutoTokenizer

# ESM2ãƒ¢ãƒ‡ãƒ«ï¼ˆã‚¿ãƒ³ãƒ‘ã‚¯è³ªåŸ‹ã‚è¾¼ã¿ç”¨ï¼‰
model_name = "facebook/esm2_t12_35M_UR50D"
model = AutoModel.from_pretrained(model_name, cache_dir="./model")
tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir="./model")

# BiomedBERTãƒ¢ãƒ‡ãƒ«ï¼ˆGO termåŸ‹ã‚è¾¼ã¿ç”¨ï¼‰
model_name = "microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract"
model = AutoModel.from_pretrained(model_name, cache_dir="./model")
tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir="./model")
```

#### ã‚ªãƒ—ã‚·ãƒ§ãƒ³B: ãƒ­ãƒ¼ã‚«ãƒ«ã®zipãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰è§£å‡

`model/zips/`ã«ãƒ¢ãƒ‡ãƒ«ã®zipãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚‹å ´åˆï¼š

```bash
# model/zipsãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ç§»å‹•
cd model/zips

# å„ãƒ¢ãƒ‡ãƒ«ã‚’è§£å‡
unzip esm2_t12_35M_UR50D.zip -d ../esm2_t12_35M_UR50D/
unzip BiomedNLP-BiomedBERT-base-uncased-abstract.zip -d ../BiomedNLP-BiomedBERT-base-uncased-abstract/

# å¿…è¦ã«å¿œã˜ã¦ä»–ã®ãƒ¢ãƒ‡ãƒ«ã‚‚è§£å‡
# unzip esm2_t30_150M_UR50D.zip -d ../esm2_t30_150M_UR50D/
# unzip esm2_t33_650M_UR50D.zip -d ../esm2_t33_650M_UR50D/

cd ../..
```

#### å¿…è¦ãªãƒ¢ãƒ‡ãƒ«

ç¾åœ¨ã®ã‚³ãƒ¼ãƒ‰ã§ä½¿ç”¨ã—ã¦ã„ã‚‹ãƒ¢ãƒ‡ãƒ«ï¼š
- `facebook/esm2_t12_35M_UR50D` - ã‚¿ãƒ³ãƒ‘ã‚¯è³ªé…åˆ—ã®åŸ‹ã‚è¾¼ã¿ç”¨
- `microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract` - GO termåŸ‹ã‚è¾¼ã¿ç”¨

**æ³¨æ„**: `model/`ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå…¨ä½“ã¯`.gitignore`ã§é™¤å¤–ã•ã‚Œã¦ã„ã¾ã™ã€‚ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã¯Gitã«ã‚³ãƒŸãƒƒãƒˆã•ã‚Œã¾ã›ã‚“ã€‚

## To Do

### âœ… ãƒ•ã‚©ãƒ«ãƒ€æ•´ç†ï¼ˆå®Œäº†ï¼‰

~~main.ipynbã«æ©Ÿèƒ½ãŒé›†ä¸­ã—ã™ãã¦ã„ã‚‹~~
~~stepã”ã¨ã«.pyãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã—ã€é–¢æ•°ã‚„ã‚¯ãƒ©ã‚¹ã‚’ç®¡ç†ã™ã‚‹~~

**å®Ÿè£…æ¸ˆã¿**: å…¨æ©Ÿèƒ½ã‚’ä»¥ä¸‹ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã«åˆ†å‰²å®Œäº†
- `data_loader.py`, `protein_embedding.py`, `go_embedding.py`
- `model.py`, `training.py`, `prediction.py`
- `hierarchical_postprocess.py`, `evaluation.py`
- `config.py` (ä¸€å…ƒç®¡ç†), `main.py` (ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ)

---

### âœ… å¾Œå‡¦ç†ã®è¿½åŠ : éšå±¤çš„è£œæ­£ (Hierarchical Postprocessing)ï¼ˆå®Œäº†ï¼‰

**å®Ÿè£…æ¸ˆã¿**: [hierarchical_postprocess.py](src/hierarchical_postprocess.py)ã§å®Œå…¨å®Ÿè£…

è©³ç´°ã¯ [docs/hierarchical_postprocessing.md](docs/hierarchical_postprocessing.md) ã‚’å‚ç…§

#### å…ƒã®è¨­è¨ˆï¼ˆå‚è€ƒï¼‰

GOã¯éšå±¤çš„ãªæ§‹é€ (DAG: æœ‰å‘éå·¡å›ã‚°ãƒ©ãƒ•)ã‚’æŒã¤ã€‚
ã‚ˆã£ã¦ã€ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬ã§ã¨ã‚ã‚‹GO, GO_aã§ã‚ã‚‹ç¢ºç‡ãŒ1ã«è¿‘ã„æ™‚ã€ãã®ä¸Šä½æ¦‚å¿µ(is_a)ã®ç¢ºç‡ã‚‚1ã«è¿‘ã„ã®ãŒè‡ªç„¶ã§ã‚ã‚‹ã€‚
é€†ã‚‚ã¾ãŸç„¶ã‚Šã§ã€GO_aã®ç¢ºç‡ãŒ0ã«è¿‘ã„æ™‚ã€ãã®ä¸‹ä½æ¦‚å¿µã‚‚0ã«è¿‘ã„ã®ãŒè‡ªç„¶ã§ã‚ã‚‹ã€‚

ã“ã®éšå±¤çš„æ•´åˆæ€§ã‚’ä¿è¨¼ã™ã‚‹ãŸã‚ã€äºˆæ¸¬å¾Œã«å¾Œå‡¦ç†ã‚’é©ç”¨ã™ã‚‹ã€‚

#### æ¡ç”¨è¨­è¨ˆ: ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ

ä»¥ä¸‹ã®2ã¤ã®è£œæ­£ã‚’é †ç•ªã«é©ç”¨ã™ã‚‹:

##### 1. **Bottom-upä¼æ’­** (å­â†’è¦ªæ–¹å‘)
- **å‡¦ç†**: ä¸‹ä½GOã®ç¢ºç‡ãŒé«˜ã„æ™‚ã€ãã®ä¸Šä½GOã®ç¢ºç‡ã‚’å¼•ãä¸Šã’ã‚‹
- **ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ **:
  ```
  ãƒˆãƒãƒ­ã‚¸ã‚«ãƒ«ã‚½ãƒ¼ãƒˆé †(é€†é †: è‘‰â†’æ ¹)ã§å‡¦ç†
  for each GO in reversed(topological_order):
      if GO has children:
          max_child_score = max(score[child] for child in children)
          score[GO] = max(score[GO], max_child_score * alpha)
  ```
- **ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿**: `alpha` (ä¼æ’­ä¿‚æ•°, æ¨å¥¨å€¤: 0.3)
- **åŠ¹æœ**: å­ãŒtrueãªã‚‰è¦ªã‚‚trueã¨ã„ã†ç”Ÿç‰©å­¦çš„ãƒ«ãƒ¼ãƒ«ã‚’ä¿è¨¼

##### 2. **Top-downæŠ‘åˆ¶** (è¦ªâ†’å­æ–¹å‘)
- **å‡¦ç†**: ä¸Šä½GOã®ç¢ºç‡ãŒä½ã„æ™‚ã€ãã®ä¸‹ä½GOã®ç¢ºç‡ã‚’å¼•ãä¸‹ã’ã‚‹
- **ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ **:
  ```
  ãƒˆãƒãƒ­ã‚¸ã‚«ãƒ«ã‚½ãƒ¼ãƒˆé †(æ ¹â†’è‘‰)ã§å‡¦ç†
  for each GO in topological_order:
      if score[GO] < threshold:
          for each child in children:
              score[child] = min(score[child], score[GO] * (1 + beta))
  ```
- **ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿**:
  - `beta` (æŠ‘åˆ¶ç·©å’Œä¿‚æ•°, æ¨å¥¨å€¤: 0.5)
  - `threshold` (æŠ‘åˆ¶é–‹å§‹é–¾å€¤, æ¨å¥¨å€¤: 0.3)
- **åŠ¹æœ**: è¦ªãŒfalseãªã®ã«å­ãŒtrueã¨ã„ã†çŸ›ç›¾ã‚’è§£æ¶ˆ

#### å®Ÿè£…ã®è©³ç´°

##### ãƒ‡ãƒ¼ã‚¿æ§‹é€ 
1. **child_to_parents**: `Dict[str, List[str]]`
   - å„GO IDã«å¯¾ã™ã‚‹è¦ªGO IDã®ãƒªã‚¹ãƒˆ
   - is_aé–¢ä¿‚ã‹ã‚‰æ§‹ç¯‰

2. **parent_to_children**: `Dict[str, List[str]]`
   - å„GO IDã«å¯¾ã™ã‚‹å­GO IDã®ãƒªã‚¹ãƒˆ
   - child_to_parentsã®é€†å¼•ãè¾æ›¸

3. **topological_order**: `List[str]`
   - ãƒˆãƒãƒ­ã‚¸ã‚«ãƒ«ã‚½ãƒ¼ãƒˆæ¸ˆã¿ã®GO IDé †åº
   - Kahn's algorithmã¾ãŸã¯DFS-basedã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§æ§‹ç¯‰

##### å‡¦ç†ãƒ•ãƒ­ãƒ¼
```
å…¥åŠ›: predictions = {GO_id: probability}

1. ã‚°ãƒ©ãƒ•æ§‹é€ æ§‹ç¯‰
   - parse_go_obo()ã‹ã‚‰is_aé–¢ä¿‚ã‚’å–å¾—
   - child_to_parents, parent_to_childenã‚’æ§‹ç¯‰
   - topological_orderã‚’è¨ˆç®—

2. Bottom-upä¼æ’­
   - reversed(topological_order)ã§å‡¦ç†
   - å„GOã«ã¤ã„ã¦å­ã®æœ€å¤§ã‚¹ã‚³ã‚¢ã§ã‚¹ã‚³ã‚¢ã‚’å¼•ãä¸Šã’

3. Top-downæŠ‘åˆ¶
   - topological_orderã§å‡¦ç†
   - è¦ªã‚¹ã‚³ã‚¢ãŒé–¾å€¤ä»¥ä¸‹ãªã‚‰å­ã‚¹ã‚³ã‚¢ã‚’æŠ‘åˆ¶

å‡ºåŠ›: è£œæ­£å¾Œã®predictions
```

#### è©•ä¾¡æ–¹æ³•

1. **Validation setã®ä½œæˆ**
   - è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚’ train/validation ã«åˆ†å‰² (ä¾‹: 80/20)
   - åŒã˜ã‚¿ãƒ³ãƒ‘ã‚¯è³ªãŒä¸¡æ–¹ã«å«ã¾ã‚Œãªã„ã‚ˆã†ã«æ³¨æ„

2. **ç²¾åº¦æ¯”è¼ƒ**
   - **Baseline**: å¾Œå‡¦ç†ãªã—
   - **Bottom-up only**: Bottom-upä¼æ’­ã®ã¿
   - **Hybrid**: Bottom-up + Top-down

3. **è©•ä¾¡æŒ‡æ¨™**
   - Precision, Recall, F1-score (é–¾å€¤0.5)
   - Average Precision (AP)
   - éšå±¤çš„æ•´åˆæ€§ã‚¹ã‚³ã‚¢: è¦ªå­é–“ã®çŸ›ç›¾ç‡

#### ãƒ•ã‚¡ã‚¤ãƒ«æ§‹æˆ (å®Ÿè£…å¾Œ)

```
src/
â”œâ”€â”€ main.py                      # ãƒ¡ã‚¤ãƒ³ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
â”œâ”€â”€ data_loader.py               # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
â”œâ”€â”€ protein_embedding.py         # ã‚¿ãƒ³ãƒ‘ã‚¯è³ªåŸ‹ã‚è¾¼ã¿
â”œâ”€â”€ go_embedding.py              # GOåŸ‹ã‚è¾¼ã¿
â”œâ”€â”€ model.py                     # JointModelå®šç¾©
â”œâ”€â”€ training.py                  # å­¦ç¿’ãƒ«ãƒ¼ãƒ—
â”œâ”€â”€ prediction.py                # äºˆæ¸¬å‡¦ç†
â”œâ”€â”€ hierarchical_postprocess.py  # éšå±¤çš„å¾Œå‡¦ç† (NEW)
â””â”€â”€ evaluation.py                # è©•ä¾¡é–¢æ•° (NEW)
```

---

### JointModelã®æ”¹å–„

JointModelã¯äºŒã¤ã®Embeddingã‚’å˜ç´”ãªç·šå‹çµåˆå±¤ä¸€ã¤ã§å¤‰æ›ã™ã‚‹ã‹ãªã‚Šå˜ç´”ãªãƒ¢ãƒ‡ãƒ«ã§ã‚ã‚‹ã€‚
ã‚‚ã†å°‘ã—è¤‡é›‘ãªãƒ¢ãƒ‡ãƒ«ã«ã™ã‚‹ã“ã¨ã§ç²¾åº¦æ”¹å–„ãŒæœŸå¾…ã§ãã‚‹ã€‚
å…·ä½“çš„ã«ã¯Transformerã®Attentionæ©Ÿæ§‹ã‚’ä¸€å±¤ã§ã„ã„ã®ã§è¿½åŠ ã§ãã‚‹ã¨è‰¯ã„ã¨è€ƒãˆã¦ã„ã‚‹ã€‚
ä½•æ•…ãªã‚‰ã€æœ¬ã‚¿ã‚¹ã‚¯ã®ç›®çš„ã¯æœ¬è³ªçš„ã«ã‚¢ãƒŸãƒé…¸é…åˆ—ã¨ãã‚Œã«å¯¾å¿œã™ã‚‹è¾æ›¸ã‚’ä½œæˆã™ã‚‹ã“ã¨ã«è¿‘ä¼¼ã§ãã€ãã‚Œã¯Attentionæ©Ÿæ§‹ã®key, valueã«å¯¾å¿œã™ã‚‹ã¨è€ƒãˆã‚‰ã‚Œã‚‹ã‹ã‚‰ã§ã‚ã‚‹ã€‚

---

### æ¨è«–æ™‚é–“çŸ­ç¸®

ç‰¹ã«esm2ã«ã‚ˆã‚‹æ¨è«–éƒ¨åˆ†ã«å¤§ããªæ™‚é–“ãŒã‹ã‹ã£ã¦ã„ã‚‹ã€‚
ã“ã®éƒ¨åˆ†ã®æ¨è«–æ™‚é–“çŸ­ç¸®ã«æˆåŠŸã™ã‚Œã°ã€ãã®åˆ†ã®ãƒªã‚½ãƒ¼ã‚¹ã‚’ä»–ã«å›ã›ã‚‹

---

### âœ… ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•´å‚™ï¼ˆå®Œäº†ï¼‰

**å®Ÿè£…æ¸ˆã¿**: `docs/`ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä»¥ä¸‹ã«å…¨ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ä½œæˆå®Œäº†

- âœ… [docs/data_loading.md](docs/data_loading.md): ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã®è©³ç´°
- âœ… [docs/embeddings.md](docs/embeddings.md): ã‚¿ãƒ³ãƒ‘ã‚¯è³ªã¨GO termã®åŸ‹ã‚è¾¼ã¿ç”Ÿæˆ
- âœ… [docs/model_architecture.md](docs/model_architecture.md): JointModelã®æ§‹é€ ã¨å­¦ç¿’
- âœ… [docs/hierarchical_postprocessing.md](docs/hierarchical_postprocessing.md): éšå±¤çš„å¾Œå‡¦ç†ã®è©³ç´°
- âœ… [docs/evaluation.md](docs/evaluation.md): è©•ä¾¡æŒ‡æ¨™ã®è©³ç´°ï¼ˆCAFA-6å…¬å¼ãƒ¡ãƒˆãƒªã‚¯ã‚¹ï¼‰

---

### ä½¿ã£ã¦ã„ãªã„inputã®ä½¿ç”¨

ç¾åœ¨ã¯inputã®ä¸­ã§ã‚‚ä½¿ã£ã¦ã„ãªã„æƒ…å ±ãŒå¤šæ•°ã‚ã‚‹ã€‚
ã“ã‚Œã‚‰ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã§æœ€çµ‚çš„ãªoutputã®ç²¾åº¦å‘ä¸ŠãŒæœŸå¾…ã§ãã‚‹ã€‚
ä½¿ç”¨æ–¹æ³•ã¯å¿…ãšã—ã‚‚æ©Ÿæ¢°å­¦ç¿’ã‚„æ·±å±¤å­¦ç¿’çš„æ‰‹æ³•ã«é™ã‚‰ãšã€å‰å‡¦ç†ã‚„å¾Œè¿°ã™ã‚‹å¾Œå‡¦ç†ã¸ã®ä½¿ç”¨ã‚‚æ¤œè¨ã•ã‚Œã‚‹ã€‚
ãŸã ã—ã€testæ™‚ã«ä½¿ãˆãªã„æƒ…å ±ã®æ‰±ã„ã«ã¯æ³¨æ„ãŒå¿…è¦ã§ã‚ã‚‹ã€‚

#### 1. `IA.tsv` (Information Accretion weights) âœ… ä¸€éƒ¨å®Ÿè£…æ¸ˆã¿
- **å†…å®¹**: å„GO termã«å¯¾ã™ã‚‹é‡è¦åº¦ã‚¹ã‚³ã‚¢(Information Accretion)
- **å®Ÿè£…æ¸ˆã¿**:
  - âœ… è©•ä¾¡æŒ‡æ¨™ã¨ã—ã¦ä½¿ç”¨ ([evaluation.py](src/evaluation.py)ã§`compute_ia_weighted_fmax()`)
  - âœ… IA-weighted Precision/Recall/Fmaxã®è¨ˆç®—ï¼ˆCAFA-6å…¬å¼ãƒ¡ãƒˆãƒªã‚¯ã‚¹ï¼‰
- **æœªå®Ÿè£…** (ä»Šå¾Œã®æ”¹å–„å€™è£œ):
  - äºˆæ¸¬æ™‚ã®é–¾å€¤èª¿æ•´: IAå€¤ãŒé«˜ã„GO termã¯äºˆæ¸¬ã‚’æ…é‡ã«è¡Œã†
  - æå¤±é–¢æ•°ã®é‡ã¿ä»˜ã‘: IAå€¤ã«å¿œã˜ã¦æå¤±ã«é‡ã¿ã‚’ä»˜ã‘ã‚‹
  - ã‚¹ã‚³ã‚¢ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³: é‡è¦ãªGO termã®äºˆæ¸¬ç¢ºç‡ã‚’èª¿æ•´
- **æ³¨æ„**: trainã¨testã®ä¸¡æ–¹ã§ä½¿ç”¨å¯èƒ½

#### 2. `testsuperset-taxon-list.tsv` (ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®ç”Ÿç‰©ç¨®æƒ…å ±)
- **å†…å®¹**: ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã®å„ã‚¿ãƒ³ãƒ‘ã‚¯è³ªã®ç”Ÿç‰©ç¨®(taxonomy ID)
- **ç”¨é€”**:
  - ç”Ÿç‰©ç¨®åˆ¥ã®ãƒ¢ãƒ‡ãƒ«é¸æŠã‚„ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«
  - ç”Ÿç‰©ç¨®å›ºæœ‰ã®GO termå‚¾å‘ã‚’æ´»ç”¨ã—ãŸå¾Œå‡¦ç†
  - ç”Ÿç‰©ç¨®æƒ…å ±ã‚’æ¡ä»¶ä»˜ã‘å¤‰æ•°ã¨ã—ã¦ä½¿ç”¨(conditional prediction)
- **æ³¨æ„**: testæ™‚ã«ä½¿ç”¨å¯èƒ½ãªã®ã§ç©æ¥µçš„ã«æ´»ç”¨ã™ã¹ã

#### 3. `train_taxonomy.tsv` (è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®ç”Ÿç‰©ç¨®æƒ…å ±)
- **å†…å®¹**: è¨“ç·´ã‚»ãƒƒãƒˆã®å„ã‚¿ãƒ³ãƒ‘ã‚¯è³ªã®ç”Ÿç‰©ç¨®(taxonomy ID)
- **ç”¨é€”**:
  - ç”Ÿç‰©ç¨®æƒ…å ±ã‚’ã‚¿ãƒ³ãƒ‘ã‚¯è³ªembeddingã«è¿½åŠ (concatenateã¾ãŸã¯cross-attention)
  - ç”Ÿç‰©ç¨®ã”ã¨ã®GO termåˆ†å¸ƒã®å­¦ç¿’
  - ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µ: ç”Ÿç‰©ç¨®æƒ…å ±ã‚’ãƒã‚¹ã‚¯ã—ãŸã‚Šæ‘‚å‹•ã•ã›ã‚‹
  - å±¤åˆ¥ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°: ç”Ÿç‰©ç¨®ã®ãƒãƒ©ãƒ³ã‚¹ã‚’è€ƒæ…®ã—ãŸå­¦ç¿’
- **æ³¨æ„**: trainã®ã¿ã§ä½¿ç”¨å¯èƒ½ã€‚testã§ã¯`testsuperset-taxon-list.tsv`ã‚’ä½¿ç”¨

#### 4. `train_terms.tsv`ã®aspectåˆ—
- **å†…å®¹**: å„GO termã®ç¨®é¡(biological_process / molecular_function / cellular_component)
- **ç”¨é€”**:
  - aspectåˆ¥ã®ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰: 3ã¤ã®ç•°ãªã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´
  - aspectåˆ¥ã®æå¤±è¨ˆç®—: å„aspectã§ç•°ãªã‚‹é‡ã¿ã‚„é–¾å€¤ã‚’ä½¿ç”¨
  - äºˆæ¸¬ã®åˆ¶ç´„: ç”Ÿç‰©å­¦çš„ã«çŸ›ç›¾ã™ã‚‹aspectã®çµ„ã¿åˆã‚ã›ã‚’æ’é™¤
  - ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«: aspectåˆ¥äºˆæ¸¬ã‚’çµ±åˆ
- **æ³¨æ„**: `go-basic.obo`ã®namespaceæƒ…å ±ã¨å¯¾å¿œã—ã¦ã„ã‚‹

#### 5. FASTAå½¢å¼ã®ã‚¢ãƒŸãƒé…¸é…åˆ—ä»¥å¤–ã®æƒ…å ±
- **å†…å®¹**: FASTAãƒ˜ãƒƒãƒ€ãƒ¼ã«å«ã¾ã‚Œã‚‹ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿(é…åˆ—IDã€ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ç­‰)
- **ç”¨é€”**:
  - é…åˆ—IDã‚’ä½¿ç”¨ã—ã¦taxonomyæƒ…å ±ã‚„GO termæƒ…å ±ã¨çµåˆ
  - ãƒ˜ãƒƒãƒ€ãƒ¼å†…ã®è¿½åŠ æƒ…å ±(ã‚‚ã—ã‚ã‚Œã°)ã®æ´»ç”¨
- **æ³¨æ„**: ç¾åœ¨ã¯é…åˆ—ã®ã¿ã‚’ä½¿ç”¨ã—ã¦ã„ã‚‹ãŒã€ãƒ˜ãƒƒãƒ€ãƒ¼æƒ…å ±ã®è§£æã‚‚æ¤œè¨

#### 6. `go-basic.obo`ã®idã¨nameä»¥å¤–ã®æƒ…å ± âœ… ä¸€éƒ¨å®Ÿè£…æ¸ˆã¿
- **å†…å®¹**: GO ontologyã®è©³ç´°æ§‹é€ 
- **å®Ÿè£…æ¸ˆã¿**:
  - âœ… **is_aé–¢ä¿‚**: éšå±¤çš„ãªå¾Œå‡¦ç† ([hierarchical_postprocess.py](src/hierarchical_postprocess.py))
    - Bottom-upä¼æ’­ã¨Top-downæŠ‘åˆ¶ã«ã‚ˆã‚‹æ•´åˆæ€§ä¿è¨¼
  - âœ… **def(å®šç¾©æ–‡)**: GO termã®nameã¨defã‚’çµåˆã—ã¦ãƒ†ã‚­ã‚¹ãƒˆembeddingã‚’ç”Ÿæˆ ([go_embedding.py](src/go_embedding.py))
  - âœ… **namespace**: parseæ™‚ã«å–å¾—æ¸ˆã¿ï¼ˆaspectåˆ¥ãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã¸ã®æ´»ç”¨ã¯æœªå®Ÿè£…ï¼‰
- **æœªå®Ÿè£…** (ä»Šå¾Œã®æ”¹å–„å€™è£œ):
  - **is_aé–¢ä¿‚**: Graph Neural Network (GNN)ã«ã‚ˆã‚‹åŸ‹ã‚è¾¼ã¿å­¦ç¿’
  - **def(å®šç¾©æ–‡)**: ã‚ˆã‚Šé«˜åº¦ãªå‰å‡¦ç†æ–¹æ³•ã®æœ€é©åŒ–
  - **namespace**: aspectåˆ¥ãƒ¢ãƒ‡ãƒ«ã®æ§‹ç¯‰
  - **relationship**: ãã®ä»–ã®é–¢ä¿‚æ€§(part_of, regulatesç­‰)ã‚’ç”¨ã„ãŸåˆ¶ç´„
  - **is_obsolete**: å»ƒæ­¢ã•ã‚ŒãŸGO termã®é™¤å¤–å‡¦ç†

#### å®Ÿè£…ã®å„ªå…ˆé †ä½(æ¨å¥¨)
1. **é«˜å„ªå…ˆåº¦**:
   - âœ… `is_a`é–¢ä¿‚: éšå±¤çš„å¾Œå‡¦ç† (å®Œäº†)
   - âœ… `IA.tsv`: è©•ä¾¡æŒ‡æ¨™ã§ã®ä½¿ç”¨ (å®Œäº†)
   - æœªå®Ÿè£…: `namespace`/`aspect`: aspectåˆ¥ãƒ¢ãƒ‡ãƒªãƒ³ã‚°
2. **ä¸­å„ªå…ˆåº¦**(æœªå®Ÿè£…):
   - `testsuperset-taxon-list.tsv`: ç”Ÿç‰©ç¨®æ¡ä»¶ä»˜ãäºˆæ¸¬
   - `train_taxonomy.tsv`: ç”Ÿç‰©ç¨®æƒ…å ±ã®embeddingçµ±åˆ
3. **ä½å„ªå…ˆåº¦**(æœªå®Ÿè£…):
   - ãã®ä»–ã®`relationship`: ã‚ˆã‚Šè¤‡é›‘ãªã‚°ãƒ©ãƒ•æ§‹é€ 
   - FASTAãƒ˜ãƒƒãƒ€ãƒ¼ã®è©³ç´°è§£æ

---

