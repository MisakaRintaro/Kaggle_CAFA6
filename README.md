# CAFA-6 Protein Function Prediction (AI-driven Development)

This repository is designed for **AI-driven development** of a protein function
prediction system for the Kaggle CAFA-6 competition.

**Competition Link**: https://www.kaggle.com/competitions/cafa-6-protein-function-prediction/overview

---

## ğŸ¯ Project Goal

Predict Gene Ontology (GO) terms for proteins using:

- Amino acid sequences
- (Optional) taxonomy / species information


---

## âŒ Non-Goals (Important)

This repository is **NOT** intended to:

- Train very large end-to-end models from scratch
- Perform heavy hyperparameter searches inside Kaggle
- Mix experimental notebook code with production code

---

## ğŸ§  AI Development Policy

When modifying or extending this repository, **AI agents must follow these rules**:

1. All reusable logic must be implemented under `src/`
2. Notebooks are for exploration only (no core logic)
3. File paths and parameters must be configurable via `configs/`
4. Cached artifacts (embeddings, models) must be saved explicitly
5. Code should favor clarity and debuggability over extreme optimization

---

## ğŸ“¦ Dataset Overview

The dataset is designed for **multi-label protein function prediction** using Gene Ontology (GO) terms.

### Key Files

| File | Description |
|------|-------------|
| `train_sequences.fasta` | Protein amino acid sequences (training) |
| `train_terms.tsv` | Ground truth GO term annotations |
| `train_taxonomy.tsv` | Taxonomy (species) information for proteins |
| `go-basic.obo` | Gene Ontology hierarchy structure |
| `testsuperset.fasta` | Test protein sequences for prediction |
| `testsuperset-taxon-list.tsv` | Taxonomy IDs for test proteins |
| `IA.tsv` | Information Accretion weights for GO terms |
| `sample_submission.tsv` | Submission format template |

### Important Notes

- **Multi-label problem**: Each protein can have multiple GO terms
- **Hierarchical structure**: GO terms are organized in an ontology
- **Taxonomy is optional**: Species information may improve predictions

### Data Statistics (from `src/main.ipynb` analysis)

**Training Sequences:**
- Number of sequences: 82,403
- Sequence length distribution:
  - Min: 16 amino acids
  - Max: 10,000+ amino acids
  - Mean: ~400-500 amino acids
  - 25th percentile: ~200 aa
  - 50th percentile (median): ~350 aa
  - 75th percentile: ~550 aa
  - 90th percentile: ~800 aa
  - 95th percentile: ~1,100 aa
  - 99th percentile: ~2,000 aa
- Long sequences (>10,000 aa): A small number exist

**Test Data:**
- Species distribution (from `testsuperset-taxon-list.tsv`):
  - Human (Homo sapiens, 9606)
  - Rat (Rattus norvegicus, 10116)
  - Rice (Oryza sativa subsp. japonica, 39947)
  - Zebrafish (Danio rerio, 7955)
  - Fruit fly (Drosophila melanogaster, 7227)

**GO Terms:**
- GO ontology structure includes:
  - `id`: GO term identifier (e.g., GO:0000001)
  - `name`: Term name (e.g., "mitochondrion inheritance")
  - `namespace`: biological_process, molecular_function, or cellular_component
  - `is_a`: Parent-child relationships in the ontology
  - `def`: Definition with references

**Submission Format:**
- The submission file contains both GO term predictions and free-text descriptions
- Two types of rows per protein:
  - GO term rows: `protein_id`, `GO:XXXXXXX`, `score` (0-1)
  - Text description rows: `protein_id`, `Text`, `score`, `description`


## ğŸ“ Project Structure

```
Kaggle_CAFA6/
â”œâ”€â”€ input/                          # Competition data files
â”‚   â””â”€â”€ cafa-6-protein-function-prediction/
â”‚       â”œâ”€â”€ Train/                  # Training data
â”‚       â”‚   â”œâ”€â”€ train_sequences.fasta
â”‚       â”‚   â”œâ”€â”€ train_terms.tsv
â”‚       â”‚   â”œâ”€â”€ train_taxonomy.tsv
â”‚       â”‚   â””â”€â”€ go-basic.obo
â”‚       â”œâ”€â”€ Test (Targets)/         # Test data
â”‚       â”‚   â”œâ”€â”€ testsuperset.fasta
â”‚       â”‚   â””â”€â”€ testsuperset-taxon-list.tsv
â”‚       â”œâ”€â”€ IA.tsv                  # GO term Information Accretion weights
â”‚       â””â”€â”€ sample_submission.tsv
â”œâ”€â”€ src/                            # Source code (modularized)
â”‚   â”œâ”€â”€ main.py                     # Main pipeline script
â”‚   â”œâ”€â”€ config.py                   # Configuration and paths
â”‚   â”œâ”€â”€ data_loader.py              # Data loading utilities
â”‚   â”œâ”€â”€ protein_embedding.py        # ESM-2 protein embedding
â”‚   â”œâ”€â”€ go_embedding.py             # BiomedBERT GO term embedding
â”‚   â”œâ”€â”€ model.py                    # JointModel architecture
â”‚   â”œâ”€â”€ training.py                 # Training loop and checkpointing
â”‚   â”œâ”€â”€ prediction.py               # Inference and submission
â”‚   â”œâ”€â”€ hierarchical_postprocess.py # Hierarchical GO postprocessing
â”‚   â”œâ”€â”€ evaluation.py               # Evaluation metrics (IA-weighted Fmax)
â”‚   â””â”€â”€ main.ipynb                  # Legacy notebook (archived)
â”œâ”€â”€ docs/                           # Documentation
â”‚   â”œâ”€â”€ data_loading.md             # Data loading details
â”‚   â”œâ”€â”€ embeddings.md               # Embedding generation
â”‚   â”œâ”€â”€ model_architecture.md       # JointModel structure
â”‚   â”œâ”€â”€ hierarchical_postprocessing.md  # Postprocessing details
â”‚   â””â”€â”€ evaluation.md               # Evaluation metrics (CAFA-6)
â”œâ”€â”€ model/                          # Pre-trained model weights (gitignored)
â”‚   â”œâ”€â”€ esm2_t12_35M_UR50D/        # ESM-2 protein embedding model
â”‚   â””â”€â”€ BiomedNLP-BiomedBERT-base-uncased-abstract-fulltext/
â”œâ”€â”€ output/                         # Generated files
â”‚   â”œâ”€â”€ embeddings/                 # Cached embeddings
â”‚   â”‚   â”œâ”€â”€ train_protein_embeddings.pt
â”‚   â”‚   â”œâ”€â”€ test_protein_embeddings.pt
â”‚   â”‚   â””â”€â”€ go_embeddings.pt
â”‚   â”œâ”€â”€ models/                     # Trained models
â”‚   â”‚   â””â”€â”€ joint_model.pt
â”‚   â””â”€â”€ submission.tsv              # Final submission file
â”œâ”€â”€ pyproject.toml                  # Project dependencies (uv)
â””â”€â”€ README.md
```

---

## ğŸ§  Modeling Approach

This repository adopts a **dual-embedding (dual-encoder) approach** for protein
function prediction.

Instead of directly classifying proteins into a fixed set of GO labels,
we embed **proteins** and **GO terms** into a shared latent space and train a model
to align them.

---

### 1. Protein Embedding

- Each protein is represented by its amino acid sequence.
- Sequences are encoded using a pretrained protein language model
  (e.g. ESM-2).
- The output is a fixed-dimensional vector representing the protein.

This embedding captures:
- sequence patterns
- evolutionary and structural signals
- biochemical properties learned from large protein databases

---

### 2. GO Term Embedding

- Each Gene Ontology (GO) term is also represented as a vector.
- GO embeddings are obtained separately (e.g. from text descriptions,
  ontology structure, or a pretrained language model).
- All GO terms are embedded into the same dimensional space.

This allows the model to:
- reason about similarities between GO terms
- generalize across related biological functions

---

### 3. Shared Latent Space

Both protein embeddings and GO embeddings are projected into a **shared latent space**
using lightweight neural networks (typically linear layers).

Let:
- `h_p` be the protein embedding
- `h_go` be the GO term embedding

Then:
- `z_p = f_p(h_p)`  
- `z_go = f_go(h_go)`

where `f_p` and `f_go` are learnable projection functions.

---

### 4. Training Objective

The model is trained to **bring related proteinâ€“GO pairs closer together**
in the latent space, while pushing unrelated pairs further apart.

Intuitively:
- If a protein is annotated with a GO term, their embeddings should be similar.
- If not, their embeddings should be dissimilar.

This alignment objective can be implemented using:
- similarity scores (e.g. dot product or cosine similarity)
- multi-label losses (e.g. BCE over proteinâ€“GO pairs)

---

### 5. Inference

At inference time:
1. Embed the protein sequence.
2. Compare it against all GO term embeddings.
3. Rank GO terms by similarity score.
4. Output GO terms and confidence scores in Kaggle submission format.

---

### Design Rationale

This embedding-based formulation offers several advantages:

- Scalability to large GO vocabularies
- Flexibility to incorporate GO semantics
- Efficient reuse of cached embeddings
- Clear separation between representation learning and prediction logic

This design is particularly suitable for:
- AI-assisted iterative development
- constrained execution environments (e.g. Kaggle notebooks)

---

## ğŸš€ Implementation Status & Pipeline

### Current Pipeline (7 Steps, 3 Phases)

The implementation is fully modularized into `src/*.py` files. The main pipeline in [src/main.py](src/main.py) is organized into **3 distinct phases** with **7 steps total**:

---

#### **Phase 1: Data Preparation (Steps 1-4)**

**Step 1: Load GO Ontology and Training Labels**
- Parse GO ontology from `go-basic.obo` to extract hierarchy (`child_to_parents`, `go_terms`)
- Load training labels from `train_terms.tsv`
- Load Information Accretion (IA) weights
- Implementation: [data_loader.py](src/data_loader.py)

**Step 2: Create GO Embeddings**
- Encode GO term names and definitions using BiomedBERT
- Model: `microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract-fulltext`
- Implementation: [go_embedding.py](src/go_embedding.py)
- Output: 768-dimensional vectors
- Cached to disk for reuse

**Step 3: Load and Embed Training Protein Sequences**
- Encode protein sequences using ESM-2 (`facebook/esm2_t12_35M_UR50D`)
- Implementation: [protein_embedding.py](src/protein_embedding.py)
- Output: 480-dimensional vectors per protein
- Cached to disk for reuse (`.pt` files in `output/embeddings/`)

**Step 4: Create Training and Validation Datasets**
- Stratified split by label count (default: 80/20 split)
- Implementation: [evaluation.py](src/evaluation.py) â†’ `split_train_validation()`
- Ensures balanced distribution of protein annotation complexity
- Creates PyTorch DataLoaders for training and validation

---

#### **Phase 2: Training and Evaluation (Step 5)**

**Step 5: Model Training and Validation**
- **Architecture**: JointModel (Dual-Encoder)
  - Protein encoder: Linear(480 â†’ 256)
  - GO encoder: Linear(768 â†’ 256)
  - Scoring: Dot product in joint space
- **Loss**: BCEWithLogitsLoss (multi-label classification)
- **Optimizer**: Adam (lr=1e-3, default 10 epochs)
- **Validation Evaluation**:
  - Evaluates on held-out validation set
  - **Compares before/after hierarchical postprocessing**:
    - Baseline metrics (without postprocessing)
    - Post-processed metrics (with hierarchical corrections)
    - Shows improvement delta (Î”)
  - Metrics: Precision, Recall, F1, Average Precision, **IA-weighted Fmax** (official CAFA-6 metric)
  - Saves comparison results to JSON file
- Implementation: [model.py](src/model.py), [training.py](src/training.py), [evaluation.py](src/evaluation.py)

---

#### **Phase 3: Inference and Submission (Steps 6-7)**

**Step 6: Test Inference**
- Load and encode test protein sequences using ESM-2
- Generate predictions for all test proteins
- Compute scores for all GO terms
- Select top-K predictions per protein (default: K=100)
- Implementation: [protein_embedding.py](src/protein_embedding.py), [prediction.py](src/prediction.py)

**Step 7: Postprocessing and Submission**
- **Hierarchical Postprocessing** (always enabled):
  - Enforces GO hierarchy constraints using hybrid approach:
    1. **Bottom-up propagation**: If child has high score, increase parent score (Î±=0.3)
    2. **Top-down suppression**: If parent has low score, decrease child score (threshold=0.3, Î²=0.5)
  - Parameters are fixed in [hierarchical_postprocess.py](src/hierarchical_postprocess.py)
  - Implementation: [hierarchical_postprocess.py](src/hierarchical_postprocess.py)
- **Submission File Creation**:
  - Generate `submission.tsv` in Kaggle submission format
  - Format: `protein_id\tGO:term\tscore` (one per line)

### Development Mode (DEV_TEST)

For faster iteration during development, set `DEV_TEST = True` in [config.py](src/config.py):

```python
# config.py
DEV_TEST = True  # Enables development mode
DEV_TEST_MAX_BATCHES = 100  # Process only first 100 batches
```

**Effects:**
- Limits ESM-2 encoding to first 100 batches (faster testing)
- Uses separate output files with `_dev` suffix to avoid overwriting production outputs
- Ideal for rapid prototyping and debugging

**Usage:**
```bash
# Development mode - quick testing
python src/main.py  # with DEV_TEST=True in config.py

# Production mode - full dataset
python src/main.py  # with DEV_TEST=False in config.py
```

### Configuration

All parameters are centralized in [src/config.py](src/config.py):

```python
# Model parameters
JOINT_DIM = 256
TRAIN_BATCH_SIZE = 16
NUM_EPOCHS = 10
LEARNING_RATE = 1e-3

# Evaluation
ENABLE_VALIDATION = True
VAL_SPLIT_RATIO = 0.2
VAL_STRATIFY_BY_LABEL_COUNT = True
```

**Hierarchical Postprocessing Parameters:**

Fixed parameters are defined in [src/hierarchical_postprocess.py](src/hierarchical_postprocess.py) and always enabled:
- `ALPHA = 0.3` (Bottom-up propagation coefficient)
- `THRESHOLD = 0.3` (Top-down suppression threshold)
- `BETA = 0.5` (Top-down suppression relaxation coefficient)

### Documentation

Detailed documentation for each component is available in the `docs/` directory:

- [Data Loading](docs/data_loading.md): FASTA parsing, GO OBO parsing, label loading
- [Embeddings](docs/embeddings.md): ESM-2 protein encoding, BiomedBERT GO encoding
- [Model Architecture](docs/model_architecture.md): JointModel structure, training, inference
- [Hierarchical Postprocessing](docs/hierarchical_postprocessing.md): Bottom-up/Top-down algorithms, parameters
- [Evaluation](docs/evaluation.md): IA-weighted Fmax, validation metrics (CAFA-6 official)

---

## ğŸ› ï¸ ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—æ‰‹é †

### 1. ç’°å¢ƒæ§‹ç¯‰

1. **uvã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«**
   ```bash
   curl -LsSf https://astral.sh/uv/install.sh | sh
   uv --version
   ```

2. **ä»®æƒ³ç’°å¢ƒä½œæˆ**
   ```bash
   uv venv
   ```

3. **ä»®æƒ³ç’°å¢ƒæœ‰åŠ¹åŒ–**
   ```bash
   source .venv/bin/activate
   ```

4. **ä¾å­˜é–¢ä¿‚ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«**
   ```bash
   uv pip install -e .
   ```

### 2. ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰

Kaggleã‹ã‚‰ã‚³ãƒ³ãƒšãƒ†ã‚£ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã€`input/cafa-6-protein-function-prediction/`ã«é…ç½®ã—ã¦ãã ã•ã„ã€‚

```bash
# Kaggle CLIã‚’ä½¿ç”¨ã™ã‚‹å ´åˆ
kaggle competitions download -c cafa-6-protein-function-prediction
unzip cafa-6-protein-function-prediction.zip -d input/cafa-6-protein-function-prediction/
```

### 3. ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

ã“ã®ãƒ¬ãƒã‚¸ãƒˆãƒªã§ã¯ã€äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã¯**Gitã§ç®¡ç†ã—ã¦ã„ã¾ã›ã‚“**ï¼ˆã‚µã‚¤ã‚ºãŒå¤§ãã„ãŸã‚ï¼‰ã€‚
ä»¥ä¸‹ã®æ‰‹é †ã§ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æº–å‚™ã—ã¦ãã ã•ã„ã€‚

#### ã‚ªãƒ—ã‚·ãƒ§ãƒ³A: Hugging Face Hubã‹ã‚‰ç›´æ¥ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆæ¨å¥¨ï¼‰

```python
from transformers import AutoModel, AutoTokenizer

# ESM2ãƒ¢ãƒ‡ãƒ«ï¼ˆã‚¿ãƒ³ãƒ‘ã‚¯è³ªåŸ‹ã‚è¾¼ã¿ç”¨ï¼‰
model_name = "facebook/esm2_t12_35M_UR50D"
model = AutoModel.from_pretrained(model_name, cache_dir="./model")
tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir="./model")

# BiomedBERTãƒ¢ãƒ‡ãƒ«ï¼ˆGO termåŸ‹ã‚è¾¼ã¿ç”¨ï¼‰
model_name = "microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract"
model = AutoModel.from_pretrained(model_name, cache_dir="./model")
tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir="./model")
```

#### ã‚ªãƒ—ã‚·ãƒ§ãƒ³B: ãƒ­ãƒ¼ã‚«ãƒ«ã®zipãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰è§£å‡

`model/zips/`ã«ãƒ¢ãƒ‡ãƒ«ã®zipãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚‹å ´åˆï¼š

```bash
# model/zipsãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ç§»å‹•
cd model/zips

# å„ãƒ¢ãƒ‡ãƒ«ã‚’è§£å‡
unzip esm2_t12_35M_UR50D.zip -d ../esm2_t12_35M_UR50D/
unzip BiomedNLP-BiomedBERT-base-uncased-abstract.zip -d ../BiomedNLP-BiomedBERT-base-uncased-abstract/

# å¿…è¦ã«å¿œã˜ã¦ä»–ã®ãƒ¢ãƒ‡ãƒ«ã‚‚è§£å‡
# unzip esm2_t30_150M_UR50D.zip -d ../esm2_t30_150M_UR50D/
# unzip esm2_t33_650M_UR50D.zip -d ../esm2_t33_650M_UR50D/

cd ../..
```

#### å¿…è¦ãªãƒ¢ãƒ‡ãƒ«

ç¾åœ¨ã®ã‚³ãƒ¼ãƒ‰ã§ä½¿ç”¨ã—ã¦ã„ã‚‹ãƒ¢ãƒ‡ãƒ«ï¼š
- `facebook/esm2_t12_35M_UR50D` - ã‚¿ãƒ³ãƒ‘ã‚¯è³ªé…åˆ—ã®åŸ‹ã‚è¾¼ã¿ç”¨
- `microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract` - GO termåŸ‹ã‚è¾¼ã¿ç”¨

**æ³¨æ„**: `model/`ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå…¨ä½“ã¯`.gitignore`ã§é™¤å¤–ã•ã‚Œã¦ã„ã¾ã™ã€‚ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã¯Gitã«ã‚³ãƒŸãƒƒãƒˆã•ã‚Œã¾ã›ã‚“ã€‚

---

## ğŸ“‹ To Do

### âœ… å®Œäº†æ¸ˆã¿ã‚¿ã‚¹ã‚¯

ä»¥ä¸‹ã®ã‚³ã‚¢æ©Ÿèƒ½ã¯å®Ÿè£…å®Œäº†ã—ã¦ã„ã¾ã™ã€‚è©³ç´°ã¯å„ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚

1. **ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«åŒ–** - 9ã¤ã®ç‹¬ç«‹ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã«æ©Ÿèƒ½åˆ†å‰²å®Œäº†
2. **7ã‚¹ãƒ†ãƒƒãƒ—ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³** - 3ãƒ•ã‚§ãƒ¼ã‚ºæ§‹é€ ï¼ˆãƒ‡ãƒ¼ã‚¿æº–å‚™â†’è¨“ç·´/è©•ä¾¡â†’æ¨è«–/æå‡ºï¼‰
3. **éšå±¤çš„å¾Œå‡¦ç†** - Bottom-up/Top-downæ‰‹æ³•å®Ÿè£…ã€å¸¸æ™‚æœ‰åŠ¹åŒ–ï¼ˆå›ºå®šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼‰
4. **è©•ä¾¡æŒ‡æ¨™** - IA-weighted Fmaxï¼ˆCAFA-6å…¬å¼ï¼‰ã€train/val splitã€before/afterå¾Œå‡¦ç†æ¯”è¼ƒ
5. **ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ** - å…¨ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®è©³ç´°ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•´å‚™

---

### æœªå®Œäº†ã‚¿ã‚¹ã‚¯

ä»¥ä¸‹ã®ã‚¿ã‚¹ã‚¯ã¯ä»Šå¾Œã®æ”¹å–„å€™è£œã§ã™ã€‚

#### 1. JointModelã®æ”¹å–„

JointModelã¯äºŒã¤ã®Embeddingã‚’å˜ç´”ãªç·šå‹çµåˆå±¤ä¸€ã¤ã§å¤‰æ›ã™ã‚‹ã‹ãªã‚Šå˜ç´”ãªãƒ¢ãƒ‡ãƒ«ã§ã‚ã‚‹ã€‚
ã‚‚ã†å°‘ã—è¤‡é›‘ãªãƒ¢ãƒ‡ãƒ«ã«ã™ã‚‹ã“ã¨ã§ç²¾åº¦æ”¹å–„ãŒæœŸå¾…ã§ãã‚‹ã€‚
å…·ä½“çš„ã«ã¯Transformerã®Attentionæ©Ÿæ§‹ã‚’ä¸€å±¤ã§ã„ã„ã®ã§è¿½åŠ ã§ãã‚‹ã¨è‰¯ã„ã¨è€ƒãˆã¦ã„ã‚‹ã€‚
ä½•æ•…ãªã‚‰ã€æœ¬ã‚¿ã‚¹ã‚¯ã®ç›®çš„ã¯æœ¬è³ªçš„ã«ã‚¢ãƒŸãƒé…¸é…åˆ—ã¨ãã‚Œã«å¯¾å¿œã™ã‚‹è¾æ›¸ã‚’ä½œæˆã™ã‚‹ã“ã¨ã«è¿‘ä¼¼ã§ãã€ãã‚Œã¯Attentionæ©Ÿæ§‹ã®key, valueã«å¯¾å¿œã™ã‚‹ã¨è€ƒãˆã‚‰ã‚Œã‚‹ã‹ã‚‰ã§ã‚ã‚‹ã€‚
ã—ã‹ã—ã€Transformerã®Attentionæ©Ÿæ§‹ã¯è¨ˆç®—ãŒé‡ãŸã„ã®ã§ã€è¦æ¤œè¨ã§ã‚ã‚‹ã€‚

#### 2. ESM-2æ¨è«–ã®é«˜é€ŸåŒ–

ç‰¹ã«ESM-2ã«ã‚ˆã‚‹æ¨è«–éƒ¨åˆ†ã«å¤§ããªæ™‚é–“ãŒã‹ã‹ã£ã¦ã„ã‚‹ã€‚ã“ã®éƒ¨åˆ†ã®æ¨è«–æ™‚é–“çŸ­ç¸®ã«æˆåŠŸã™ã‚Œã°ã€ãã®åˆ†ã®ãƒªã‚½ãƒ¼ã‚¹ã‚’ä»–ã«å›ã›ã‚‹ã€‚

---

### è¿½åŠ ãƒ‡ãƒ¼ã‚¿ã®æ´»ç”¨

ç¾åœ¨ã¯inputã®ä¸­ã§ã‚‚ä½¿ã£ã¦ã„ãªã„æƒ…å ±ãŒå¤šæ•°ã‚ã‚‹ã€‚
ã“ã‚Œã‚‰ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã§æœ€çµ‚çš„ãªoutputã®ç²¾åº¦å‘ä¸ŠãŒæœŸå¾…ã§ãã‚‹ã€‚
ä½¿ç”¨æ–¹æ³•ã¯å¿…ãšã—ã‚‚æ©Ÿæ¢°å­¦ç¿’ã‚„æ·±å±¤å­¦ç¿’çš„æ‰‹æ³•ã«é™ã‚‰ãšã€å‰å‡¦ç†ã‚„å¾Œè¿°ã™ã‚‹å¾Œå‡¦ç†ã¸ã®ä½¿ç”¨ã‚‚æ¤œè¨ã•ã‚Œã‚‹ã€‚
ãŸã ã—ã€testæ™‚ã«ä½¿ãˆãªã„æƒ…å ±ã®æ‰±ã„ã«ã¯æ³¨æ„ãŒå¿…è¦ã§ã‚ã‚‹ã€‚

#### 1. `IA.tsv` (Information Accretion weights) âœ… ä¸€éƒ¨å®Ÿè£…æ¸ˆã¿
- **å†…å®¹**: å„GO termã«å¯¾ã™ã‚‹é‡è¦åº¦ã‚¹ã‚³ã‚¢(Information Accretion)
- **å®Ÿè£…æ¸ˆã¿**:
  - âœ… è©•ä¾¡æŒ‡æ¨™ã¨ã—ã¦ä½¿ç”¨ ([evaluation.py](src/evaluation.py)ã§`compute_ia_weighted_fmax()`)
  - âœ… IA-weighted Precision/Recall/Fmaxã®è¨ˆç®—ï¼ˆCAFA-6å…¬å¼ãƒ¡ãƒˆãƒªã‚¯ã‚¹ï¼‰
- **æœªå®Ÿè£…** (ä»Šå¾Œã®æ”¹å–„å€™è£œ):
  - äºˆæ¸¬æ™‚ã®é–¾å€¤èª¿æ•´: IAå€¤ãŒé«˜ã„GO termã¯äºˆæ¸¬ã‚’æ…é‡ã«è¡Œã†
  - æå¤±é–¢æ•°ã®é‡ã¿ä»˜ã‘: IAå€¤ã«å¿œã˜ã¦æå¤±ã«é‡ã¿ã‚’ä»˜ã‘ã‚‹
  - ã‚¹ã‚³ã‚¢ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³: é‡è¦ãªGO termã®äºˆæ¸¬ç¢ºç‡ã‚’èª¿æ•´
- **æ³¨æ„**: trainã¨testã®ä¸¡æ–¹ã§ä½¿ç”¨å¯èƒ½

#### 2. `testsuperset-taxon-list.tsv` (ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®ç”Ÿç‰©ç¨®æƒ…å ±)
- **å†…å®¹**: ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã®å„ã‚¿ãƒ³ãƒ‘ã‚¯è³ªã®ç”Ÿç‰©ç¨®(taxonomy ID)
- **ç”¨é€”**:
  - ç”Ÿç‰©ç¨®åˆ¥ã®ãƒ¢ãƒ‡ãƒ«é¸æŠã‚„ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«
  - ç”Ÿç‰©ç¨®å›ºæœ‰ã®GO termå‚¾å‘ã‚’æ´»ç”¨ã—ãŸå¾Œå‡¦ç†
  - ç”Ÿç‰©ç¨®æƒ…å ±ã‚’æ¡ä»¶ä»˜ã‘å¤‰æ•°ã¨ã—ã¦ä½¿ç”¨(conditional prediction)
- **æ³¨æ„**: testæ™‚ã«ä½¿ç”¨å¯èƒ½ãªã®ã§ç©æ¥µçš„ã«æ´»ç”¨ã™ã¹ã

#### 3. `train_taxonomy.tsv` (è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®ç”Ÿç‰©ç¨®æƒ…å ±)
- **å†…å®¹**: è¨“ç·´ã‚»ãƒƒãƒˆã®å„ã‚¿ãƒ³ãƒ‘ã‚¯è³ªã®ç”Ÿç‰©ç¨®(taxonomy ID)
- **ç”¨é€”**:
  - ç”Ÿç‰©ç¨®æƒ…å ±ã‚’ã‚¿ãƒ³ãƒ‘ã‚¯è³ªembeddingã«è¿½åŠ (concatenateã¾ãŸã¯cross-attention)
  - ç”Ÿç‰©ç¨®ã”ã¨ã®GO termåˆ†å¸ƒã®å­¦ç¿’
  - ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µ: ç”Ÿç‰©ç¨®æƒ…å ±ã‚’ãƒã‚¹ã‚¯ã—ãŸã‚Šæ‘‚å‹•ã•ã›ã‚‹
  - å±¤åˆ¥ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°: ç”Ÿç‰©ç¨®ã®ãƒãƒ©ãƒ³ã‚¹ã‚’è€ƒæ…®ã—ãŸå­¦ç¿’
- **æ³¨æ„**: trainã®ã¿ã§ä½¿ç”¨å¯èƒ½ã€‚testã§ã¯`testsuperset-taxon-list.tsv`ã‚’ä½¿ç”¨

#### 4. `train_terms.tsv`ã®aspectåˆ—
- **å†…å®¹**: å„GO termã®ç¨®é¡(biological_process / molecular_function / cellular_component)
- **ç”¨é€”**:
  - aspectåˆ¥ã®ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰: 3ã¤ã®ç•°ãªã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´
  - aspectåˆ¥ã®æå¤±è¨ˆç®—: å„aspectã§ç•°ãªã‚‹é‡ã¿ã‚„é–¾å€¤ã‚’ä½¿ç”¨
  - äºˆæ¸¬ã®åˆ¶ç´„: ç”Ÿç‰©å­¦çš„ã«çŸ›ç›¾ã™ã‚‹aspectã®çµ„ã¿åˆã‚ã›ã‚’æ’é™¤
  - ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«: aspectåˆ¥äºˆæ¸¬ã‚’çµ±åˆ
- **æ³¨æ„**: `go-basic.obo`ã®namespaceæƒ…å ±ã¨å¯¾å¿œã—ã¦ã„ã‚‹

#### 5. FASTAå½¢å¼ã®ã‚¢ãƒŸãƒé…¸é…åˆ—ä»¥å¤–ã®æƒ…å ±
- **å†…å®¹**: FASTAãƒ˜ãƒƒãƒ€ãƒ¼ã«å«ã¾ã‚Œã‚‹ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿(é…åˆ—IDã€ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ç­‰)
- **ç”¨é€”**:
  - é…åˆ—IDã‚’ä½¿ç”¨ã—ã¦taxonomyæƒ…å ±ã‚„GO termæƒ…å ±ã¨çµåˆ
  - ãƒ˜ãƒƒãƒ€ãƒ¼å†…ã®è¿½åŠ æƒ…å ±(ã‚‚ã—ã‚ã‚Œã°)ã®æ´»ç”¨
- **æ³¨æ„**: ç¾åœ¨ã¯é…åˆ—ã®ã¿ã‚’ä½¿ç”¨ã—ã¦ã„ã‚‹ãŒã€ãƒ˜ãƒƒãƒ€ãƒ¼æƒ…å ±ã®è§£æã‚‚æ¤œè¨

#### 6. `go-basic.obo`ã®idã¨nameä»¥å¤–ã®æƒ…å ± âœ… ä¸€éƒ¨å®Ÿè£…æ¸ˆã¿
- **å†…å®¹**: GO ontologyã®è©³ç´°æ§‹é€ 
- **å®Ÿè£…æ¸ˆã¿**:
  - âœ… **is_aé–¢ä¿‚**: éšå±¤çš„ãªå¾Œå‡¦ç† ([hierarchical_postprocess.py](src/hierarchical_postprocess.py))
    - Bottom-upä¼æ’­ã¨Top-downæŠ‘åˆ¶ã«ã‚ˆã‚‹æ•´åˆæ€§ä¿è¨¼
  - âœ… **def(å®šç¾©æ–‡)**: GO termã®nameã¨defã‚’çµåˆã—ã¦ãƒ†ã‚­ã‚¹ãƒˆembeddingã‚’ç”Ÿæˆ ([go_embedding.py](src/go_embedding.py))
  - âœ… **namespace**: parseæ™‚ã«å–å¾—æ¸ˆã¿ï¼ˆaspectåˆ¥ãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã¸ã®æ´»ç”¨ã¯æœªå®Ÿè£…ï¼‰
- **æœªå®Ÿè£…** (ä»Šå¾Œã®æ”¹å–„å€™è£œ):
  - **is_aé–¢ä¿‚**: Graph Neural Network (GNN)ã«ã‚ˆã‚‹åŸ‹ã‚è¾¼ã¿å­¦ç¿’
  - **def(å®šç¾©æ–‡)**: ã‚ˆã‚Šé«˜åº¦ãªå‰å‡¦ç†æ–¹æ³•ã®æœ€é©åŒ–
  - **namespace**: aspectåˆ¥ãƒ¢ãƒ‡ãƒ«ã®æ§‹ç¯‰
  - **relationship**: ãã®ä»–ã®é–¢ä¿‚æ€§(part_of, regulatesç­‰)ã‚’ç”¨ã„ãŸåˆ¶ç´„
  - **is_obsolete**: å»ƒæ­¢ã•ã‚ŒãŸGO termã®é™¤å¤–å‡¦ç†

**å®Ÿè£…ã®å„ªå…ˆé †ä½(æ¨å¥¨):**

1. **é«˜å„ªå…ˆåº¦**: `namespace`/`aspect`ã‚’ç”¨ã„ãŸaspectåˆ¥ãƒ¢ãƒ‡ãƒªãƒ³ã‚°
2. **ä¸­å„ªå…ˆåº¦**: ç”Ÿç‰©ç¨®æƒ…å ±ã®æ´»ç”¨ (`train_taxonomy.tsv`, `testsuperset-taxon-list.tsv`)
3. **ä¸­å„ªå…ˆåº¦**: IAé‡ã¿ã‚’ç”¨ã„ãŸæå¤±é–¢æ•°ã®æ”¹å–„
4. **ä½å„ªå…ˆåº¦**: ãã®ä»–ã®GO relationshipã€FASTAãƒ˜ãƒƒãƒ€ãƒ¼è§£æ

---

